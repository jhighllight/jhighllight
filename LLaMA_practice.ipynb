{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meuVOHOB5Fn6",
        "outputId": "649cf8e0-34e7-4fbb-b4e9-26cca706e278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 9.3626\n",
            "Epoch 2/10, Loss: 8.6779\n",
            "Epoch 3/10, Loss: 7.9754\n",
            "Epoch 4/10, Loss: 7.3647\n",
            "Epoch 5/10, Loss: 6.7910\n",
            "Epoch 6/10, Loss: 6.4936\n",
            "Epoch 7/10, Loss: 6.1352\n",
            "Epoch 8/10, Loss: 5.9113\n",
            "Epoch 9/10, Loss: 5.7506\n",
            "Epoch 10/10, Loss: 5.5866\n",
            "Sample output: tensor([9052, 1197, 1197, 9721, 9721, 3964, 1548, 1982, 3964, 1982, 9538, 9538,\n",
            "        1197, 1197, 1982, 1982, 4443, 1982, 1197, 9721, 9538, 9538, 9721, 9721,\n",
            "        6934, 9538, 8393, 1982, 9538, 1197, 9721, 9538])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Transformer\n",
        "\n",
        "# 모델 정의\n",
        "class LLaMAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "        super(LLaMAModel, self).__init__()\n",
        "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src, tgt = self.embedding(src), self.embedding(tgt)\n",
        "        output = self.transformer(src, tgt)\n",
        "        return self.fc(output)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = 10000\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# 모델 및 옵티마이저 초기화\n",
        "model = LLaMAModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 임의의 데이터 생성\n",
        "src = torch.randint(0, vocab_size, (10, 32))  # 10 sequences of length 32\n",
        "tgt = torch.randint(0, vocab_size, (10, 32))\n",
        "tgt_output = torch.randint(0, vocab_size, (10, 32))\n",
        "\n",
        "# 학습\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, tgt)\n",
        "    loss = criterion(output.view(-1, vocab_size), tgt_output.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Sample output:\", torch.argmax(output[0], dim=1))"
      ]
    }
  ]
}